#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Unified toolkit for exporting chat logs, training causal LMs, and running utilities."""
from __future__ import annotations

import argparse
import json
import logging
import math
import os
import re
import textwrap
import time
from dataclasses import dataclass
from datetime import datetime
from string import ascii_letters, digits
from typing import Iterable, List, Optional

logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')

# ============================================================
# Ø§Ù„Ù‚Ø³Ù… (Ø£): Ù…ÙØµØ¯ÙÙ‘Ø± Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©
# ============================================================
SUPPORTED_ROLES = {"user": "Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…", "assistant": "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯", "system": "Ø§Ù„Ù†Ø¸Ø§Ù…"}


@dataclass
class Message:
    role: str
    content: str
    timestamp: Optional[str] = None  # ISO8601 Ø¥Ø°Ø§ Ù…ØªÙˆÙØ±


def load_messages(path: str) -> List[Message]:
    import csv

    ext = os.path.splitext(path)[1].lower()
    msgs: List[Message] = []
    if ext == ".csv":
        with open(path, newline='', encoding="utf-8") as f:
            sample = f.read(4096)
            f.seek(0)
            try:
                dialect = csv.Sniffer().sniff(sample)
            except csv.Error:
                dialect = csv.excel
            reader = csv.DictReader(f, dialect=dialect)
            for r in reader:
                role = (r.get("role") or "").strip()
                content = (r.get("content") or "")
                ts = r.get("timestamp") or r.get("time")
                if role or content:
                    msgs.append(Message(role=role, content=content, timestamp=ts))
    elif ext == ".jsonl":
        with open(path, encoding="utf-8") as f:
            for i, line in enumerate(f, 1):
                if not line.strip():
                    continue
                try:
                    o = json.loads(line)
                except json.JSONDecodeError as e:
                    logging.warning("JSONL ØºÙŠØ± ØµØ§Ù„Ø­ ÙÙŠ Ø§Ù„Ø³Ø·Ø± %d: %s", i, e)
                    continue
                msgs.append(
                    Message(
                        role=(o.get("role") or ""),
                        content=(o.get("content") or ""),
                        timestamp=o.get("timestamp") or o.get("time"),
                    )
                )
    elif ext == ".json":
        with open(path, encoding="utf-8") as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError as e:
                raise ValueError(f"JSON ØºÙŠØ± ØµØ§Ù„Ø­: {e}")
        items = data.get("messages") if isinstance(data, dict) else data
        if not isinstance(items, list):
            raise ValueError("Ù…Ù„Ù JSON ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‚Ø§Ø¦Ù…Ø© Ø±Ø³Ø§Ø¦Ù„ Ø£Ùˆ ÙƒØ§Ø¦Ù†Ù‹Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…ÙØªØ§Ø­ 'messages'.")
        for i, o in enumerate(items, 1):
            if not isinstance(o, dict):
                logging.warning("Ø¹Ù†ØµØ± ØºÙŠØ± Ù‚Ø§Ù…ÙˆØ³ÙŠ Ø¹Ù†Ø¯ %dØŒ Ø³ÙŠØªÙ… ØªØ¬Ø§ÙˆØ²Ù‡.", i)
                continue
            msgs.append(
                Message(
                    role=(o.get("role") or ""),
                    content=(o.get("content") or ""),
                    timestamp=o.get("timestamp") or o.get("time"),
                )
            )
    else:
        raise ValueError("ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…. Ø§Ø³ØªØ®Ø¯Ù… .csv Ø£Ùˆ .jsonl Ø£Ùˆ .json")
    if not msgs:
        logging.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ø±Ø³Ø§Ø¦Ù„ ØµØ§Ù„Ø­Ø© Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©.")
    return msgs


def _local_iso() -> str:
    return datetime.now().astimezone().isoformat(timespec="seconds")


MD_SPECIALS = ("*", "_", "#", ">", "`")


def md_escape_first_char(s: str) -> str:
    out_lines = []
    for ln in s.replace("\r", "").split("\n"):
        if ln and ln[0] in MD_SPECIALS:
            out_lines.append("\\" + ln)
        else:
            out_lines.append(ln)
    return "\n".join(out_lines)


def to_markdown(msgs: List[Message], title: str = "Ø³Ø¬Ù„ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©") -> str:
    lines = [f"# {title}", f"_ØªÙ… Ø§Ù„ØªØµØ¯ÙŠØ±: {_local_iso()}_\n"]
    for i, m in enumerate(msgs, 1):
        role_key = (m.role or "").lower()
        prefix = f"**{SUPPORTED_ROLES.get(role_key, m.role or 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')}**"
        ts = f"\n> _{m.timestamp}_" if getattr(m, "timestamp", None) else ""
        lines.append(f"### {i}. {prefix}{ts}")
        lines.append(md_escape_first_char(m.content) + "\n")
    return "\n".join(lines)


def save_markdown(md: str, outpath: str) -> None:
    with open(outpath, "w", encoding="utf-8") as f:
        f.write(md)


def save_docx(msgs: List[Message], outpath: str, title: str = "Ø³Ø¬Ù„ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©", font_name: str = "Arial") -> None:
    from docx import Document
    from docx.oxml import OxmlElement
    from docx.oxml.ns import qn
    from docx.shared import Pt

    def set_paragraph_rtl(paragraph):
        p = paragraph._p
        pPr = p.get_or_add_pPr()
        bidi = OxmlElement('w:bidi')
        bidi.set(qn('w:val'), '1')
        pPr.append(bidi)

    doc = Document()
    doc.add_heading(title, 0)
    style = doc.styles["Normal"]
    style.font.name = font_name
    style._element.rPr.rFonts.set(qn('w:eastAsia'), font_name)
    style.font.size = Pt(11)

    for i, m in enumerate(msgs, 1):
        role_key = (m.role or "").lower()
        role_title = SUPPORTED_ROLES.get(role_key, m.role or 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ')
        h = doc.add_heading(f"{i}. {role_title}", level=2)
        set_paragraph_rtl(h)
        p = doc.add_paragraph(m.content)
        set_paragraph_rtl(p)

    doc.save(outpath)


def save_xlsx(msgs: List[Message], outpath: str) -> None:
    import pandas as pd

    df = pd.DataFrame([{"role": m.role, "content": m.content, "timestamp": m.timestamp} for m in msgs])
    df.index = df.index + 1
    df.index.name = "idx"
    df.to_excel(outpath, index=True)


def save_pdf(md_text: str, outpath: str, font_paths: Optional[Iterable[str]] = None) -> None:
    from reportlab.lib.pagesizes import A4
    from reportlab.lib.units import mm
    from reportlab.pdfbase import pdfmetrics
    from reportlab.pdfbase.ttfonts import TTFont
    from reportlab.pdfgen import canvas

    width, height = A4
    c = canvas.Canvas(outpath, pagesize=A4)

    chosen_font = "Helvetica"

    def reshape_default(s: str) -> str:
        return s

    reshape = reshape_default
    try:
        import arabic_reshaper  # type: ignore
        from bidi.algorithm import get_display  # type: ignore

        def reshape(s: str) -> str:  # type: ignore[redefined-outer-name]
            try:
                return get_display(arabic_reshaper.reshape(s))
            except Exception:
                return s
    except Exception:
        logging.warning("Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ arabic-reshaper/python-bidiØ› Ø³ÙŠÙØ·Ø¨Ø¹ Ø§Ù„Ù†Øµ Ø¯ÙˆÙ† ØªØ´ÙƒÙŠÙ„ RTL.")

    candidates = [
        "Amiri-Regular.ttf",
        "Amiri.ttf",
        "Arial.ttf",
        "NotoNaskhArabic-Regular.ttf",
        "NotoSansArabic-Regular.ttf",
    ]
    if font_paths:
        candidates = list(font_paths) + candidates

    for fp in candidates:
        if os.path.isfile(fp):
            try:
                pdfmetrics.registerFont(TTFont("CustomArabic", fp))
                chosen_font = "CustomArabic"
                break
            except Exception:
                continue

    c.setFont(chosen_font, 12)

    x_left = 20 * mm
    x_right = width - 20 * mm
    y = height - 20 * mm
    line_h = 6 * mm
    wrap_chars = 95

    for raw_para in md_text.split("\n"):
        para = raw_para.rstrip("\n")
        lines = textwrap.wrap(para, width=wrap_chars) if para.strip() else [""]
        for line in lines:
            s = reshape(line)
            if s and any('Ø€' <= ch <= 'Û¿' for ch in s):
                c.drawRightString(x_right, y, s)
            else:
                c.drawString(x_left, y, s)
            y -= line_h
            if y < 20 * mm:
                c.showPage()
                c.setFont(chosen_font, 12)
                y = height - 20 * mm
    c.save()


def cmd_export(args: argparse.Namespace) -> None:
    os.makedirs(args.outdir, exist_ok=True)
    msgs = load_messages(args.inp)
    md = to_markdown(msgs, title=args.title)
    formats = {f.strip().lower() for f in args.formats.split(',') if f.strip()}
    if 'md' in formats:
        save_markdown(md, os.path.join(args.outdir, 'transcript.md'))
        logging.info('ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Markdown')
    if 'docx' in formats:
        try:
            save_docx(msgs, os.path.join(args.outdir, 'transcript.docx'), title=args.title, font_name=args.font)
            logging.info('ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ DOCX')
        except Exception as e:
            logging.exception('ÙØ´Ù„ DOCX: %s', e)
    if 'xlsx' in formats:
        try:
            save_xlsx(msgs, os.path.join(args.outdir, 'transcript.xlsx'))
            logging.info('ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ XLSX')
        except Exception as e:
            logging.exception('ÙØ´Ù„ XLSX: %s', e)
    if 'pdf' in formats:
        try:
            extra_fonts = [args.pdf_font] if args.pdf_font else None
            save_pdf(md, os.path.join(args.outdir, 'transcript.pdf'), font_paths=extra_fonts)
            logging.info('ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ PDF')
        except Exception as e:
            logging.exception('ÙØ´Ù„ PDF: %s', e)
    print('ØªÙ… Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰', args.outdir)


# ============================================================
# Ø§Ù„Ù‚Ø³Ù… (Ø¨): ØªØ¯Ø±ÙŠØ¨ ÙˆØªÙ‚ÙŠÙŠÙ… Causal LM
# ============================================================

def build_datasets(
    train_files: List[str],
    valid_files: Optional[List[str]],
    valid_ratio: float,
    tokenizer,
    block_size: int,
    test_files: Optional[List[str]] = None,
    test_ratio: Optional[float] = None,
):
    from datasets import DatasetDict, load_dataset

    data_files = {"train": train_files}
    if valid_files:
        data_files["validation"] = valid_files
    if test_files:
        data_files["test"] = test_files

    if valid_files or test_files:
        raw = load_dataset("text", data_files=data_files)
        dsd = DatasetDict(raw)
    else:
        raw_all = load_dataset("text", data_files={"train": train_files})
        split = raw_all["train"].train_test_split(test_size=valid_ratio, seed=42)
        train_part, valid_part = split["train"], split["test"]
        if test_ratio and test_ratio > 0:
            split2 = train_part.train_test_split(test_size=test_ratio, seed=42)
            train_part, test_part = split2["train"], split2["test"]
            dsd = DatasetDict({"train": train_part, "validation": valid_part, "test": test_part})
        else:
            dsd = DatasetDict({"train": train_part, "validation": valid_part})

    def tokenize_function(examples):
        return tokenizer(examples["text"])  # Ø¨Ù„Ø§ padding Ø«Ø§Ø¨Øª Ù‡Ù†Ø§

    tokenized = dsd.map(tokenize_function, batched=True, remove_columns=["text"])  # type: ignore[arg-type]

    def group_texts(examples):
        concatenated = {k: sum(examples[k], []) for k in examples.keys()}
        total_length = len(concatenated["input_ids"])
        total_length = (total_length // block_size) * block_size
        result = {
            k: [t[i:i + block_size] for i in range(0, total_length, block_size)]
            for k, t in concatenated.items()
        }
        result["labels"] = result["input_ids"].copy()
        return result

    tokenized = tokenized.map(group_texts, batched=True, num_proc=1)
    return tokenized


class SampleGenerationCallback:
    """Callback Ø¨Ø³ÙŠØ· Ù„ØªÙˆÙ„ÙŠØ¯ Ø¹ÙŠÙ†Ø§Øª Ù†ØµÙŠØ© ÙÙŠ Ù†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ø­Ù‚Ø¨Ø©."""

    def __init__(
        self,
        tokenizer,
        prompts: List[str],
        outdir: str,
        max_length: int = 128,
        temperature: float = 0.8,
        top_p: float = 0.9,
        top_k: int = 50,
        num_return_sequences: int = 1,
        no_repeat_ngram_size: int = 2,
    ):
        from transformers import TrainerCallback

        class _CB(TrainerCallback):
            def __init__(self, outer: "SampleGenerationCallback"):
                self.outer = outer

            def on_epoch_end(self, args, state, control, **kwargs):  # type: ignore[override]
                import torch

                model = kwargs.get('model')
                tokenizer = self.outer.tokenizer
                outdir = self.outer.outdir
                os.makedirs(outdir, exist_ok=True)
                if model is None:
                    return
                device = model.device
                gen_kwargs = dict(
                    max_length=self.outer.max_length,
                    do_sample=True,
                    temperature=self.outer.temperature,
                    top_p=self.outer.top_p,
                    top_k=self.outer.top_k,
                    no_repeat_ngram_size=self.outer.no_repeat_ngram_size,
                    num_return_sequences=self.outer.num_return_sequences,
                    pad_token_id=tokenizer.eos_token_id,
                )
                for pi, prompt in enumerate(self.outer.prompts, 1):
                    inputs = tokenizer(prompt, return_tensors='pt').to(device)
                    with torch.no_grad():
                        out = model.generate(**inputs, **gen_kwargs)
                    texts = [tokenizer.decode(seq, skip_special_tokens=True) for seq in out]
                    epoch_id = int(state.epoch) if state.epoch is not None else 0
                    path = os.path.join(outdir, f"epoch_{epoch_id:02d}_prompt_{pi}.txt")
                    with open(path, 'w', encoding='utf-8') as f:
                        f.write(f"# Prompt:\n{prompt}\n\n# Outputs:\n\n")
                        for i, t in enumerate(texts, 1):
                            f.write(f"=== {i} ===\n{t}\n\n")

        self.tokenizer = tokenizer
        self.prompts = prompts
        self.outdir = outdir
        self.max_length = max_length
        self.temperature = temperature
        self.top_p = top_p
        self.top_k = top_k
        self.num_return_sequences = num_return_sequences
        self.no_repeat_ngram_size = no_repeat_ngram_size
        self.cb = _CB(self)


def cmd_train(args: argparse.Namespace) -> None:
    from transformers import (
        AutoModelForCausalLM,
        AutoTokenizer,
        DataCollatorForLanguageModeling,
        EarlyStoppingCallback,
        Trainer,
        TrainingArguments,
    )

    os.makedirs(args.output_dir, exist_ok=True)

    tokenizer = AutoTokenizer.from_pretrained(args.model_name, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    tokenized = build_datasets(
        train_files=args.train_files,
        valid_files=args.valid_files,
        valid_ratio=args.valid_ratio,
        tokenizer=tokenizer,
        block_size=args.block_size,
        test_files=args.test_files,
        test_ratio=args.test_ratio,
    )

    model = AutoModelForCausalLM.from_pretrained(args.model_name)
    if getattr(model.config, "pad_token_id", None) is None:
        model.config.pad_token_id = tokenizer.pad_token_id

    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    eval_strategy = "steps" if args.eval_steps and args.eval_steps > 0 else "epoch"
    save_strategy = "steps" if args.save_steps and args.save_steps > 0 else eval_strategy

    training_args = TrainingArguments(
        output_dir=args.output_dir,
        overwrite_output_dir=True,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.grad_accum,
        learning_rate=args.lr,
        weight_decay=args.weight_decay,
        lr_scheduler_type=args.lr_scheduler_type,
        warmup_ratio=args.warmup_ratio,
        fp16=args.fp16,
        bf16=args.bf16,
        save_steps=args.save_steps if save_strategy == "steps" else None,
        eval_steps=args.eval_steps if eval_strategy == "steps" else None,
        save_total_limit=args.save_total_limit,
        logging_steps=50,
        report_to=["tensorboard"],
        logging_dir=os.path.join(args.output_dir, "tb_logs"),
        evaluation_strategy=eval_strategy,
        save_strategy=save_strategy,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        seed=args.seed,
        gradient_checkpointing=args.gradient_checkpointing,
        eval_accumulation_steps=args.eval_accumulation_steps,
        push_to_hub=args.push_to_hub,
        hub_model_id=args.hub_model_id,
    )

    def compute_metrics(eval_pred):
        preds, labels = eval_pred
        if hasattr(preds, 'ndim') and preds.ndim == 3:
            import numpy as np

            preds = preds.argmax(-1)
        mask = labels != -100
        correct = (preds[mask] == labels[mask]).sum()
        total = mask.sum()
        accuracy = float(correct) / float(total) if total > 0 else 0.0
        return {"accuracy": accuracy}

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized["train"],
        eval_dataset=tokenized.get("validation"),
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    if args.early_stopping_patience and args.early_stopping_patience > 0:
        trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=args.early_stopping_patience))

    sample_prompts: List[str] = []
    if args.sample_prompts_file and os.path.isfile(args.sample_prompts_file):
        with open(args.sample_prompts_file, encoding='utf-8') as f:
            for ln in f:
                ln = ln.strip()
                if ln:
                    sample_prompts.append(ln)
    if args.sample_prompt:
        sample_prompts.extend(args.sample_prompt)

    if sample_prompts:
        sgc = SampleGenerationCallback(
            tokenizer=tokenizer,
            prompts=sample_prompts,
            outdir=os.path.join(args.output_dir, 'samples'),
            max_length=args.sample_max_length,
            temperature=args.sample_temperature,
            top_p=args.sample_top_p,
            top_k=args.sample_top_k,
            num_return_sequences=args.sample_num_return_sequences,
            no_repeat_ngram_size=args.sample_no_repeat_ngram_size,
        )
        trainer.add_callback(sgc.cb)

    trainer.train()

    eval_metrics = trainer.evaluate()
    eval_loss = eval_metrics.get("eval_loss")
    ppl = math.exp(eval_loss) if isinstance(eval_loss, (int, float)) else float('nan')
    eval_metrics["perplexity"] = ppl

    trainer.save_model(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)

    metrics_val_path = os.path.join(args.output_dir, "metrics_val.json")
    with open(metrics_val_path, "w", encoding="utf-8") as f:
        json.dump({k: (float(v) if isinstance(v, (int, float)) else v) for k, v in eval_metrics.items()}, f, ensure_ascii=False, indent=2)

    if tokenized.get("test") is not None:
        test_metrics = trainer.evaluate(eval_dataset=tokenized["test"])
        test_loss = test_metrics.get("eval_loss")
        test_ppl = math.exp(test_loss) if isinstance(test_loss, (int, float)) else float('nan')
        test_metrics["perplexity"] = test_ppl
        metrics_test_path = os.path.join(args.output_dir, "metrics_test.json")
        with open(metrics_test_path, "w", encoding="utf-8") as f:
            json.dump({k: (float(v) if isinstance(v, (int, float)) else v) for k, v in test_metrics.items()}, f, ensure_ascii=False, indent=2)
        logging.info("ØªÙ… Ø­ÙØ¸ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¥Ù„Ù‰ %s", metrics_test_path)
    else:
        test_metrics = None

    print("\nğŸ“Š Validation Metrics:")
    for k, v in eval_metrics.items():
        print(f"- {k}: {v}")
    if test_metrics is not None:
        print("\nğŸ§ª Test Metrics:")
        for k, v in test_metrics.items():
            print(f"- {k}: {v}")
    print("\nâœ… Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ¯Ø±ÙŠØ¨. Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙÙˆØ¸ ÙÙŠ:", args.output_dir)
    print("ğŸ“ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªØ­Ù‚Ù‚:", metrics_val_path)
    if test_metrics is not None:
        print("ğŸ“ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:", metrics_test_path)
    print("ğŸ§¾ Ø¹ÙŠÙ†Ø§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯ (Ø¥Ù† ÙˆÙØ¬Ø¯Øª):", os.path.join(args.output_dir, 'samples'))
    print("ğŸ§ª TensorBoard logs:", os.path.join(args.output_dir, "tb_logs"))


# ============================================================
# Ø§Ù„Ù‚Ø³Ù… (Ø¬): ØªÙˆÙ„ÙŠØ¯ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆÙ‡Ø§Øª
# ============================================================

def cmd_generate(args: argparse.Namespace) -> None:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer

    model_path = args.model_path or args.model_name
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(model_path)

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = tokenizer.pad_token_id

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    inputs = tokenizer(args.prompt, return_tensors="pt").to(device)

    gen_kwargs = dict(
        max_length=args.max_length,
        do_sample=not args.use_beam_search,
        temperature=args.temperature,
        top_p=args.top_p,
        top_k=args.top_k,
        repetition_penalty=args.repetition_penalty,
        no_repeat_ngram_size=args.no_repeat_ngram_size,
        num_return_sequences=args.num_return_sequences,
        pad_token_id=tokenizer.eos_token_id,
    )
    if args.use_beam_search:
        gen_kwargs.update(dict(num_beams=args.num_beams, early_stopping=True))

    with torch.no_grad():
        output = model.generate(**inputs, **gen_kwargs)

    texts = [tokenizer.decode(seq, skip_special_tokens=True) for seq in output]
    print("\nğŸ“ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:")
    for i, t in enumerate(texts, 1):
        print(f"\n=== # {i} ===\n{t}")


# ============================================================
# Ø§Ù„Ù‚Ø³Ù… (Ø¯): Ù…Ø«Ø§Ù„ Ø¯ÙˆØ§Ù„ spells (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
# ============================================================

def is_powerful(magic: str) -> bool:
    powerful_set = {"Sourcery", "More Sourcery"}
    return magic in powerful_set


def find_more(magicks: List[str]) -> List[str]:
    return [m for m in magicks if is_powerful(m)]


def print_all(spells: List[str]) -> None:
    for s in spells:
        print(s)


def cmd_spells(args: argparse.Namespace) -> None:
    items = [s.strip() for s in (args.list or '').split(',') if s.strip()]
    strong = find_more(items)
    print("âœ¨ Powerful spells:")
    print_all(strong)


# ============================================================
# Ø§Ù„Ù‚Ø³Ù… (Ù‡Ù€): ÙˆØ§Ø¬Ù‡Ø© Gradio (Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª ÙˆØ§Ù„ØµØ­Ø© Ø§Ù„Ø¬Ù†Ø³ÙŠØ©)
# ============================================================

def _ascii_ratio(s: str) -> float:
    if not s:
        return 0.0
    letters = sum(ch in ascii_letters for ch in s)
    digits_count = sum(ch in digits for ch in s)
    return (letters + digits_count) / max(len(s), 1)


AR_KEYWORDS = [
    "Ø¬Ù†Ø³", "Ø­Ù…ÙŠÙ…ÙŠ", "Ø­Ù…ÙŠÙ…ÙŠØ©", "Ø¹Ù„Ø§Ù‚Ø©", "Ø¹Ù„Ø§Ù‚Ø§Øª", "Ø²ÙˆØ¬", "Ø²ÙˆØ¬Ø©", "Ø²ÙˆØ§Ø¬", "Ø·Ù„Ø§Ù‚",
    "Ø§Ù†ÙØµØ§Ù„", "Ø®ÙŠØ§Ù†Ø©", "ØºÙŠØ±Ø©", "ØªØ¹Ø§Ø±Ù", "Ù…ÙˆØ§Ø¹Ø¯Ø©", "ØªÙˆØ§ØµÙ„", "Ø­ÙˆØ§Ø±", "Ø§ØªØµØ§Ù„",
    "Ù…ÙˆØ§ÙÙ‚Ø©", "Ø±Ø¶Ø§", "Ø­Ø¯ÙˆØ¯", "Ø«Ù‚Ø©", "Ù‚Ø±Ø¨", "Ù…Ø´Ø§Ø¹Ø±", "Ø­Ù…ÙŠÙ…", "Ø±ØºØ¨Ø©", "Ù…ØªØ¹Ø©",
    "Ù†Ø´ÙˆØ©", "Ø§ÙˆØ±Ø¬Ø§Ø²Ù…", "Ù†Ù‚Ø§Ø´", "ØªÙˆØ§ÙÙ‚", "Ù‚Ø§Ø¨Ù„ÙŠØ©", "Ø®Ù„Ù„ Ø¬Ù†Ø³ÙŠ", "Ø¶Ø¹Ù Ø¬Ù†Ø³ÙŠ",
    "Ù‚Ø°Ù", "Ø§Ù†ØªØµØ§Ø¨", "ØªØ´Ø­ÙŠÙ…", "Ù…Ø²Ù„Ù‚", "ØµØ­Ø© Ø¬Ù†Ø³ÙŠØ©", "ØªØ«Ù‚ÙŠÙ Ø¬Ù†Ø³ÙŠ", "Ø§Ø³ØªÙ…Ù†Ø§Ø¡",
    "ØªØ®ÙŠÙ„Ø§Øª", "Ø®ÙŠØ§Ù„ Ø¬Ù†Ø³ÙŠ", "Ø¢Ù…Ù†Ø©", "Ù…Ù…Ø§Ø±Ø³Ø© Ø¢Ù…Ù†Ø©", "Ø¹Ù„Ø§Ø¬ Ø²ÙˆØ¬ÙŠ", "Ø¹Ù„Ø§Ø¬ Ø¬Ù†Ø³ÙŠ",
    "Ø§Ø±Ø´Ø§Ø¯", "Ø§Ø³ØªØ´Ø§Ø±Ø©", "Ø¹Ù„Ø§Ø¬ Ù†ÙØ³ÙŠ"
]
EN_KEYWORDS = [
    "sex", "sexual", "intimacy", "relationship", "marriage", "divorce", "dating",
    "consent", "boundaries", "trust", "communication", "orgasm", "pleasure",
    "arousal", "erectile", "ejaculation", "libido", "vibrator", "toy", "lube",
    "sex education", "sex therapy", "couples therapy", "anxiety"
]
ALL_PHRASES = sorted({*AR_KEYWORDS, *EN_KEYWORDS}, key=len, reverse=True)
KEYWORD_PATTERN = re.compile(r"\b(" + "|".join(map(re.escape, ALL_PHRASES)) + r")\b", re.IGNORECASE)
AR_LETTERS = re.compile(r"[Ø€-Û¿]")


def is_relevant(question: str) -> bool:
    q = (question or "").strip()
    if not q:
        return False
    has_ar = bool(AR_LETTERS.search(q))
    has_en_like = _ascii_ratio(q) >= 0.25
    relevant = bool(KEYWORD_PATTERN.search(q))
    return relevant and (has_ar or has_en_like)


def cmd_serve(args: argparse.Namespace) -> None:
    try:
        import gradio as gr
    except Exception as e:
        raise RuntimeError("Gradio ØºÙŠØ± Ù…Ø«Ø¨Øª. Ø«Ø¨Ù‘Øª gradio Ø£ÙˆÙ„Ø§Ù‹: pip install gradio") from e
    try:
        from huggingface_hub import InferenceClient
    except Exception as e:
        raise RuntimeError("huggingface_hub ØºÙŠØ± Ù…Ø«Ø¨Øª. Ø«Ø¨Ù‘Øª Ø§Ù„Ø­Ø²Ù…Ø©: pip install huggingface_hub") from e

    api_token = os.getenv(args.api_token_env or "")
    if not api_token:
        raise RuntimeError(f"Ù…ÙÙ‚ÙˆØ¯ Ø§Ù„ØªÙˆÙƒÙ† ÙÙŠ Ø§Ù„Ù…ØªØºÙŠÙ‘Ø± Ø§Ù„Ø¨ÙŠØ¦ÙŠ '{args.api_token_env}'.")

    client = InferenceClient(args.model_id, token=api_token)

    def _chat_with_retries(messages, max_tokens, temperature, top_p, retries=3, delay=0.8):
        last_error = None
        for attempt in range(1, retries + 1):
            try:
                stream = client.chat_completion(
                    messages=messages,
                    max_tokens=max_tokens,
                    stream=True,
                    temperature=temperature,
                    top_p=top_p,
                )
                for chunk in stream:
                    token = None
                    try:
                        token = chunk.choices[0].delta.content  # type: ignore[attr-defined]
                    except Exception:
                        pass
                    if token:
                        yield token
                return
            except Exception as e:
                last_error = e
                if attempt < retries:
                    time.sleep(delay * attempt)
                else:
                    raise last_error

    def respond(message, history, system_message, max_tokens, temperature, top_p):
        history = history or []
        if len(history) == 0:
            yield {"role": "assistant", "content": "Ù…Ø±Ø­Ø¨Ù‹Ø§. Ø§Ø³Ø£Ù„ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ù† Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø£Ùˆ Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¬Ù†Ø³ÙŠØ©. Ø³Ø£Ø¬ÙŠØ¨ Ø¨Ø§Ø®ØªØµØ§Ø± ÙˆÙˆØ¶ÙˆØ­."}
            return
        if not is_relevant(message):
            yield {"role": "assistant", "content": "Ù…Ù† ÙØ¶Ù„Ùƒ Ø§Ø³Ø£Ù„ Ø¹Ù† Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§ØªØŒ Ø§Ù„ØªÙˆØ§ØµÙ„ØŒ Ø§Ù„Ù…ÙˆØ§ÙÙ‚Ø©ØŒ Ø£Ùˆ Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¬Ù†Ø³ÙŠØ©. Ø³Ø£Ø¬ÙŠØ¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."}
            return
        messages = [{"role": "system", "content": system_message}]
        messages.extend(history)
        messages.append({"role": "user", "content": message})
        response_text = ""
        try:
            for token in _chat_with_retries(messages, max_tokens, temperature, top_p):
                response_text += token
                yield {"role": "assistant", "content": response_text}
        except Exception as e:  # pragma: no cover - network errors
            yield {"role": "assistant", "content": f"Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {type(e).__name__}. Ø­Ø§ÙˆÙ„ Ù„Ø§Ø­Ù‚Ù‹Ø§."}

    demo = gr.ChatInterface(
        fn=respond,
        additional_inputs=[
            gr.Textbox(
                value="Ø£Ù†Øª Ù…Ø³ØªØ´Ø§Ø± Ø¹Ù„Ø§Ù‚Ø§Øª ÙˆØµØ­Ø© Ø¬Ù†Ø³ÙŠØ© Ù…Ø­ØªØ±Ù. Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨ÙˆØ¶ÙˆØ­ ÙˆØ­Ø²Ù… ÙˆØ¨Ù„Ø§ Ø£Ø­ÙƒØ§Ù…. Ù‚Ø¯Ù‘Ù… ØªÙˆØ§Ø²Ù†Ù‹Ø§ Ø¨ÙŠÙ† Ø§Ù„Ø¹Ù„Ù… ÙˆØ§Ù„Ø¹Ù…Ù„ÙŠØ©.",
                label="Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…",
            ),
            gr.Slider(minimum=50, maximum=1024, value=512, step=1, label="Ø£Ù‚ØµÙ‰ Ø¹Ø¯Ø¯ Ø±Ù…ÙˆØ²", visible=False),
            gr.Slider(minimum=0.1, maximum=1.0, value=0.7, step=0.05, label="Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©", visible=False),
            gr.Slider(minimum=0.1, maximum=1.0, value=0.9, step=0.05, label="Top-p", visible=False),
        ],
        chatbot=gr.Chatbot(type="messages", label="Ù…Ø³ØªØ´Ø§Ø± Ø¹Ù„Ø§Ù‚Ø§Øª ÙˆØµØ­Ø© Ø¬Ù†Ø³ÙŠØ©"),
        title="Ù…Ø³ØªØ´Ø§Ø± Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª ÙˆØ§Ù„ØµØ­Ø© Ø§Ù„Ø¬Ù†Ø³ÙŠØ©",
        description="Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ø¹Ù„ÙˆÙ…Ø§ØªÙŠØ© ÙˆÙ…Ø¨Ø§Ø´Ø±Ø©.",
    )
    demo.launch(server_name=args.host, server_port=args.port, show_api=False)


# CLI Ø±Ø¦ÙŠØ³ÙŠ
# ============================================================

def build_parser() -> argparse.ArgumentParser:
    ap = argparse.ArgumentParser(description="Ø£Ø¯Ø§Ø© Ù…ÙˆØ­Ù‘Ø¯Ø©: ØªØµØ¯ÙŠØ± Ø¯Ø±Ø¯Ø´Ø§Øª + ØªØ¯Ø±ÙŠØ¨/ØªÙ‚ÙŠÙŠÙ… + ØªÙˆÙ„ÙŠØ¯")
    sub = ap.add_subparsers(dest="command", required=True)

    # export
    ape = sub.add_parser("export", help="ØªØµØ¯ÙŠØ± Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©")
    ape.add_argument("--in", dest="inp", required=True, help="Ù…Ù„Ù Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ .csv Ø£Ùˆ .jsonl Ø£Ùˆ .json")
    ape.add_argument("--out", dest="outdir", required=True, help="Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬")
    ape.add_argument("--title", dest="title", default="Ø³Ø¬Ù„ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©", help="Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚")
    ape.add_argument("--formats", default="md,docx,xlsx,pdf", help="md,docx,xlsx,pdf")
    ape.add_argument("--pdf-font", dest="pdf_font", default=None, help="Ù…Ø³Ø§Ø± Ø®Ø· TTF Ù„Ù…Ù„Ù PDF")
    ape.add_argument("--font", dest="font", default="Arial", help="Ø§Ø³Ù… Ø®Ø· DOCX")
    ape.set_defaults(func=cmd_export)

    # train
    apt = sub.add_parser("train", help="ØªØ¯Ø±ÙŠØ¨ Causal LM Ù…Ø¹ ØªÙ‚ÙŠÙŠÙ…")
    apt.add_argument("--train_files", nargs="+", required=True)
    apt.add_argument("--valid_files", nargs="+", default=None)
    apt.add_argument("--valid_ratio", type=float, default=0.05)
    apt.add_argument("--test_files", nargs="+", default=None)
    apt.add_argument("--test_ratio", type=float, default=0.0)
    apt.add_argument("--model_name", default="EleutherAI/gpt-neo-125M")
    apt.add_argument("--output_dir", default="./results")
    apt.add_argument("--block_size", type=int, default=256)
    apt.add_argument("--epochs", type=float, default=3.0)
    apt.add_argument("--batch_size", type=int, default=8)
    apt.add_argument("--grad_accum", type=int, default=1)
    apt.add_argument("--lr", type=float, default=5e-5)
    apt.add_argument("--weight_decay", type=float, default=0.01)
    apt.add_argument("--lr_scheduler_type", default="linear")
    apt.add_argument("--warmup_ratio", type=float, default=0.0)
    apt.add_argument("--fp16", action="store_true")
    apt.add_argument("--bf16", action="store_true")
    apt.add_argument("--save_steps", type=int, default=0)
    apt.add_argument("--eval_steps", type=int, default=0)
    apt.add_argument("--save_total_limit", type=int, default=2)
    apt.add_argument("--seed", type=int, default=42)
    apt.add_argument("--early_stopping_patience", type=int, default=0)
    apt.add_argument("--gradient_checkpointing", action="store_true")
    apt.add_argument("--eval_accumulation_steps", type=int, default=1)
    apt.add_argument("--push_to_hub", action="store_true")
    apt.add_argument("--hub_model_id", default=None)
    apt.add_argument("--sample_prompts_file", default=None, help="Ù…Ù„Ù Ù†ØµÙŠ ÙŠØ­ÙˆÙŠ prompt ÙÙŠ ÙƒÙ„ Ø³Ø·Ø±")
    apt.add_argument("--sample_prompt", action='append', default=None, help="ÙŠÙ…ÙƒÙ† ØªÙ…Ø±ÙŠØ± Ù‡Ø°Ø§ Ø§Ù„Ø®ÙŠØ§Ø± Ø¹Ø¯Ø© Ù…Ø±Ø§Øª Ù„Ø¥Ø¶Ø§ÙØ© Ø¨Ø±ÙˆÙ…Ø¨ØªØ§Øª")
    apt.add_argument("--sample_max_length", type=int, default=128)
    apt.add_argument("--sample_temperature", type=float, default=0.8)
    apt.add_argument("--sample_top_p", type=float, default=0.9)
    apt.add_argument("--sample_top_k", type=int, default=50)
    apt.add_argument("--sample_num_return_sequences", type=int, default=1)
    apt.add_argument("--sample_no_repeat_ngram_size", type=int, default=2)

    apt.set_defaults(func=cmd_train)

    # generate
    apg = sub.add_parser("generate", help="ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ (Ù…Ø­Ù„ÙŠ Ø£Ùˆ Hub)")
    apg.add_argument("--prompt", required=True)
    apg.add_argument("--model_path", default=None, help="Ù…Ø³Ø§Ø± Ù…Ø¬Ù„Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙÙˆØ¸ Ù…Ø­Ù„ÙŠÙ‹Ø§")
    apg.add_argument("--model_name", default="EleutherAI/gpt-neo-125M", help="Ø§Ø³Ù… Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Hub Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ model_path")
    apg.add_argument("--max_length", type=int, default=128)
    apg.add_argument("--temperature", type=float, default=0.8)
    apg.add_argument("--top_p", type=float, default=0.9)
    apg.add_argument("--top_k", type=int, default=50)
    apg.add_argument("--repetition_penalty", type=float, default=1.1)
    apg.add_argument("--no_repeat_ngram_size", type=int, default=2)
    apg.add_argument("--num_return_sequences", type=int, default=1)
    apg.add_argument("--use_beam_search", action="store_true")
    apg.add_argument("--num_beams", type=int, default=4)
    apg.set_defaults(func=cmd_generate)

    # spells (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    aps = sub.add_parser("spells", help="Ù…Ø«Ø§Ù„ Ø¨Ø³ÙŠØ· Ù„Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©")
    aps.add_argument("--list", default="Sourcery,Alchemy,More Sourcery")
    aps.set_defaults(func=cmd_spells)

    # serve (Gradio)
    apv = sub.add_parser("serve", help="ØªØ´ØºÙŠÙ„ ÙˆØ§Ø¬Ù‡Ø© Gradio Ù„Ù„Ù…Ø³ØªØ´Ø§Ø±")
    apv.add_argument("--model_id", default="HuggingFaceH4/zephyr-7b-beta")
    apv.add_argument("--api_token_env", default="seksuoloog_sexologist_bot", help="Ø§Ø³Ù… Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø§Ù„ØªÙˆÙƒÙ†")
    apv.add_argument("--host", default="0.0.0.0")
    apv.add_argument("--port", type=int, default=7860)
    apv.set_defaults(func=cmd_serve)

    return ap


def main():
    parser = build_parser()
    args = parser.parse_args()
    args.func(args)


if __name__ == "__main__":
    main()
